{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall scikit-learn -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.3Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.3.29 requires numpy>=1.26.2; python_version < \"3.13\", but you have numpy 1.24.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached numpy-1.24.3-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pandas==2.1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib==3.8.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.1.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.1.4) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.1.4) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.8.2) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.8.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.8.2) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.8.2) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.8.2) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.8.2) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.8.2) (3.1.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.3.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.3.2) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.3.2) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.16.0)\n",
      "Using cached numpy-1.24.3-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.3\n",
      "    Uninstalling numpy-2.3.3:\n",
      "      Successfully uninstalled numpy-2.3.3\n",
      "Successfully installed numpy-1.24.3\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy==1.24.3 pandas==2.1.4 matplotlib==3.8.2 scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IsolationForest\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     ArrowDtype,\n\u001b[0;32m     62\u001b[0m     Int8Dtype,\n\u001b[0;32m     63\u001b[0m     Int16Dtype,\n\u001b[0;32m     64\u001b[0m     Int32Dtype,\n\u001b[0;32m     65\u001b[0m     Int64Dtype,\n\u001b[0;32m     66\u001b[0m     UInt8Dtype,\n\u001b[0;32m     67\u001b[0m     UInt16Dtype,\n\u001b[0;32m     68\u001b[0m     UInt32Dtype,\n\u001b[0;32m     69\u001b[0m     UInt64Dtype,\n\u001b[0;32m     70\u001b[0m     Float32Dtype,\n\u001b[0;32m     71\u001b[0m     Float64Dtype,\n\u001b[0;32m     72\u001b[0m     CategoricalDtype,\n\u001b[0;32m     73\u001b[0m     PeriodDtype,\n\u001b[0;32m     74\u001b[0m     IntervalDtype,\n\u001b[0;32m     75\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     76\u001b[0m     StringDtype,\n\u001b[0;32m     77\u001b[0m     BooleanDtype,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     NA,\n\u001b[0;32m     80\u001b[0m     isna,\n\u001b[0;32m     81\u001b[0m     isnull,\n\u001b[0;32m     82\u001b[0m     notna,\n\u001b[0;32m     83\u001b[0m     notnull,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     Index,\n\u001b[0;32m     86\u001b[0m     CategoricalIndex,\n\u001b[0;32m     87\u001b[0m     RangeIndex,\n\u001b[0;32m     88\u001b[0m     MultiIndex,\n\u001b[0;32m     89\u001b[0m     IntervalIndex,\n\u001b[0;32m     90\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     91\u001b[0m     DatetimeIndex,\n\u001b[0;32m     92\u001b[0m     PeriodIndex,\n\u001b[0;32m     93\u001b[0m     IndexSlice,\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     NaT,\n\u001b[0;32m     96\u001b[0m     Period,\n\u001b[0;32m     97\u001b[0m     period_range,\n\u001b[0;32m     98\u001b[0m     Timedelta,\n\u001b[0;32m     99\u001b[0m     timedelta_range,\n\u001b[0;32m    100\u001b[0m     Timestamp,\n\u001b[0;32m    101\u001b[0m     date_range,\n\u001b[0;32m    102\u001b[0m     bdate_range,\n\u001b[0;32m    103\u001b[0m     Interval,\n\u001b[0;32m    104\u001b[0m     interval_range,\n\u001b[0;32m    105\u001b[0m     DateOffset,\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     to_numeric,\n\u001b[0;32m    108\u001b[0m     to_datetime,\n\u001b[0;32m    109\u001b[0m     to_timedelta,\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     Flags,\n\u001b[0;32m    112\u001b[0m     Grouper,\n\u001b[0;32m    113\u001b[0m     factorize,\n\u001b[0;32m    114\u001b[0m     unique,\n\u001b[0;32m    115\u001b[0m     value_counts,\n\u001b[0;32m    116\u001b[0m     NamedAgg,\n\u001b[0;32m    117\u001b[0m     array,\n\u001b[0;32m    118\u001b[0m     Categorical,\n\u001b[0;32m    119\u001b[0m     set_eng_float_format,\n\u001b[0;32m    120\u001b[0m     Series,\n\u001b[0;32m    121\u001b[0m     DataFrame,\n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124manomaly_detection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCaseauditdetails_LIS 3.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(r\"C:\\Users\\Hp\\anomaly_detection\\Caseauditdetails_LIS 3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split implementation across modular cells below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "Found existing installation: pandas 2.1.4\n",
      "Uninstalling pandas-2.1.4:\n",
      "  Successfully uninstalled pandas-2.1.4\n",
      "Found existing installation: scipy 1.15.3\n",
      "Uninstalling scipy-1.15.3:\n",
      "  Successfully uninstalled scipy-1.15.3\n",
      "Found existing installation: scikit-learn 1.3.2\n",
      "Uninstalling scikit-learn-1.3.2:\n",
      "  Successfully uninstalled scikit-learn-1.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 51.2/61.0 kB 2.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 51.2/61.0 kB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 61.0/61.0 kB 406.4 kB/s eta 0:00:00\n",
      "Collecting pandas==2.1.4\n",
      "  Downloading pandas-2.1.4-cp311-cp311-win_amd64.whl.metadata (18 kB)\n",
      "Collecting scipy==1.11.4\n",
      "  Downloading scipy-1.11.4-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.4 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 51.2/60.4 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 51.2/60.4 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 51.2/60.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 60.4/60.4 kB 321.5 kB/s eta 0:00:00\n",
      "Collecting scikit-learn==1.3.2\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.1.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.1.4) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.1.4) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.3.2) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn==1.3.2) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.16.0)\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.4/15.8 MB 7.6 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.5/15.8 MB 4.2 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.7/15.8 MB 3.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.4/15.8 MB 5.7 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.4/15.8 MB 5.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.0/15.8 MB 5.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.5/15.8 MB 5.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.9/15.8 MB 5.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/15.8 MB 5.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.7/15.8 MB 5.6 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 4.1/15.8 MB 5.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.4/15.8 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.8/15.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.3/15.8 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.8/15.8 MB 5.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.2/15.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.7/15.8 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.1/15.8 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.5/15.8 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.9/15.8 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.2/15.8 MB 5.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.6/15.8 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.0/15.8 MB 5.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.4/15.8 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.8/15.8 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.1/15.8 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.4/15.8 MB 5.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.8/15.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.2/15.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.6/15.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.0/15.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.8 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.9/15.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.6/15.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.8/15.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.2/15.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.5/15.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.9/15.8 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.8 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.4/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading pandas-2.1.4-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/10.6 MB 7.3 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.5/10.6 MB 6.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/10.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.1/10.6 MB 5.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.7/10.6 MB 5.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.2/10.6 MB 5.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/10.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/10.6 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.3/10.6 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.9/10.6 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.2/10.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.6/10.6 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.9/10.6 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.3/10.6 MB 5.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.6/10.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.9/10.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.0/10.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.9/10.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.3/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.4/10.6 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.3/10.6 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.3/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.4-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "   ---------------------------------------- 0.0/44.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/44.1 MB 16.7 MB/s eta 0:00:03\n",
      "    --------------------------------------- 1.0/44.1 MB 11.1 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.4/44.1 MB 9.7 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.6/44.1 MB 9.0 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.6/44.1 MB 6.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 1.8/44.1 MB 6.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.2/44.1 MB 5.9 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 2.5/44.1 MB 6.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.9/44.1 MB 6.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 3.2/44.1 MB 6.0 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.5/44.1 MB 6.0 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.7/44.1 MB 5.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.9/44.1 MB 6.0 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 4.3/44.1 MB 5.9 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 4.7/44.1 MB 6.0 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 4.9/44.1 MB 5.9 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 5.1/44.1 MB 5.9 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 5.2/44.1 MB 5.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 5.5/44.1 MB 5.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 5.8/44.1 MB 5.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 6.0/44.1 MB 5.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 6.2/44.1 MB 5.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 6.4/44.1 MB 5.5 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.7/44.1 MB 5.5 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.9/44.1 MB 5.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 7.2/44.1 MB 5.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 7.4/44.1 MB 5.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 7.6/44.1 MB 5.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 8.0/44.1 MB 5.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 8.4/44.1 MB 5.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 8.8/44.1 MB 5.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 9.2/44.1 MB 5.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 9.7/44.1 MB 5.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 10.0/44.1 MB 5.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 10.6/44.1 MB 5.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 11.0/44.1 MB 5.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 11.5/44.1 MB 5.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 11.9/44.1 MB 5.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 12.2/44.1 MB 5.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 12.6/44.1 MB 5.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 13.0/44.1 MB 5.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 13.5/44.1 MB 5.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 14.0/44.1 MB 5.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 14.4/44.1 MB 6.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 14.9/44.1 MB 6.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 15.4/44.1 MB 6.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 15.7/44.1 MB 6.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 16.1/44.1 MB 6.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 16.5/44.1 MB 6.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 17.0/44.1 MB 6.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 17.1/44.1 MB 6.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 17.5/44.1 MB 6.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 17.8/44.1 MB 6.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 18.1/44.1 MB 6.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 18.5/44.1 MB 6.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 18.9/44.1 MB 6.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 19.3/44.1 MB 6.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 19.8/44.1 MB 6.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 20.1/44.1 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 20.5/44.1 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 20.8/44.1 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 21.1/44.1 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 21.3/44.1 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 21.6/44.1 MB 6.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 22.0/44.1 MB 6.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 22.3/44.1 MB 6.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 22.7/44.1 MB 6.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 23.0/44.1 MB 6.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 23.4/44.1 MB 6.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 23.8/44.1 MB 6.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 24.2/44.1 MB 6.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 24.6/44.1 MB 6.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 25.0/44.1 MB 6.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 25.5/44.1 MB 6.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 25.8/44.1 MB 6.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 25.9/44.1 MB 6.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 26.4/44.1 MB 6.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 26.8/44.1 MB 6.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.2/44.1 MB 6.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.5/44.1 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 28.0/44.1 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 28.3/44.1 MB 7.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 28.8/44.1 MB 7.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 29.1/44.1 MB 7.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 29.4/44.1 MB 6.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 29.7/44.1 MB 6.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.0/44.1 MB 6.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.4/44.1 MB 6.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.7/44.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 31.2/44.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 31.7/44.1 MB 7.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 32.0/44.1 MB 7.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 32.4/44.1 MB 7.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 32.8/44.1 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.3/44.1 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.8/44.1 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.4/44.1 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.9/44.1 MB 7.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.4/44.1 MB 7.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.7/44.1 MB 7.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 36.2/44.1 MB 7.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 36.4/44.1 MB 7.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 36.9/44.1 MB 7.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 37.3/44.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.6/44.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.8/44.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.2/44.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.6/44.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.9/44.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.1/44.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.5/44.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.0/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.3/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.8/44.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.1/44.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.5/44.1 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.9/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.3/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.7/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.3/44.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/44.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.1/44.1 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.3.2-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "   ---------------------------------------- 0.0/9.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/9.2 MB 7.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/9.2 MB 6.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.2/9.2 MB 6.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.0/9.2 MB 7.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.3/9.2 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.5/9.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.9/9.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.1/9.2 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.5/9.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.8/9.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.1/9.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.4/9.2 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.6/9.2 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.8/9.2 MB 6.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.0/9.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.2/9.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.4/9.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.7/9.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.9/9.2 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.2/9.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.5/9.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.7/9.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.9/9.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.2/9.2 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.5/9.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.1/9.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.5/9.2 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.8/9.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.2/9.2 MB 5.1 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, scipy, pandas, scikit-learn\n",
      "Successfully installed numpy-1.26.4 pandas-2.1.4 scikit-learn-1.3.2 scipy-1.11.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y numpy pandas scipy scikit-learn\n",
    "%pip install numpy==1.26.4 pandas==2.1.4 scipy==1.11.4 scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from faker import Faker\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and configuration\n",
    "fake = Faker()\n",
    "roles = ['Pathologist Assistant', 'Admin', 'Lab Technician']\n",
    "actions_allowed = {\n",
    "    'Pathologist Assistant': ['Logged In', 'Viewed Final Report', 'Viewed Case Information', 'Revised as Addendum',\n",
    "                              'Added Supplemental', 'Modified Diagnosis', 'Previewed Report'],\n",
    "    'Admin': ['Logged In', 'Viewed Final Report', 'Modified Diagnosis', 'Added Supplemental', 'Create User', 'Delete User'],\n",
    "    'Lab Technician': ['Logged In', 'Viewed Case Information', 'Collected Sample']\n",
    "}\n",
    "sensitive_actions = {'Revised as Addendum', 'Modified Diagnosis', 'Create User', 'Delete User'}\n",
    "cities_coords = {\n",
    "    'Hudson': (42.3770, -71.5661),\n",
    "    'Boston': (42.3601, -71.0589),\n",
    "    'New York': (40.7128, -74.0060),\n",
    "    'Chicago': (41.8781, -87.6298),\n",
    "    'San Francisco': (37.7749, -122.4194)\n",
    "}\n",
    "cities = list(cities_coords.keys())\n",
    "user_regions = ['US', 'EU', 'ASIA']\n",
    "organizations = ['Leominster Dermatology, LLP', 'Quantum Pathology Inc', 'Health Lab Solutions']\n",
    "DATA_PATH = r\"C:\\Users\\Hp\\anomaly_detection\\Caseauditdetails_LIS 3.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud detection logic\n",
    "def determine_fraud_status(role, action, time_taken, audit_date, user_region):\n",
    "    \"\"\"\n",
    "    Determine if a record should be labeled as fraud based on various patterns\n",
    "    Returns 1 for fraud, 0 for not fraud\n",
    "    \"\"\"\n",
    "    fraud_indicators = []\n",
    "    \n",
    "    # Pattern 1: Role violation (action not allowed for role)\n",
    "    if role in actions_allowed:\n",
    "        allowed_actions = actions_allowed[role]\n",
    "        if action not in allowed_actions:\n",
    "            fraud_indicators.append(\"role_violation\")\n",
    "    \n",
    "    # Pattern 2: Suspicious timing (too fast execution)\n",
    "    base_time = {'Pathologist Assistant': 4000, 'Admin': 6000, 'Lab Technician': 5000}\n",
    "    expected_min = base_time.get(role, 5000) // 10\n",
    "    if time_taken < expected_min:\n",
    "        fraud_indicators.append(\"too_fast\")\n",
    "    \n",
    "    # Pattern 3: After hours activity with sensitive actions\n",
    "    sensitive_actions = {'Revised as Addendum', 'Modified Diagnosis', 'Create User', 'Delete User'}\n",
    "    hour = audit_date.hour\n",
    "    if (hour < 6 or hour > 22) and action in sensitive_actions:\n",
    "        fraud_indicators.append(\"after_hours_sensitive\")\n",
    "    \n",
    "    # Pattern 4: Multiple region activity (simplified - random chance for now)\n",
    "    if user_region != 'US' and action in sensitive_actions:\n",
    "        fraud_indicators.append(\"foreign_sensitive\")\n",
    "    \n",
    "    # Pattern 5: Admin actions outside normal pattern\n",
    "    if role != 'Admin' and action in ['Create User', 'Delete User']:\n",
    "        fraud_indicators.append(\"unauthorized_admin\")\n",
    "    \n",
    "    # Determine fraud based on indicators\n",
    "    # 15% base fraud rate with multiple patterns possible\n",
    "    if len(fraud_indicators) >= 2:\n",
    "        return 1  # Definite fraud\n",
    "    elif len(fraud_indicators) == 1:\n",
    "        return 1 if random.random() < 0.7 else 0  # 70% chance of fraud\n",
    "    else:\n",
    "        return 1 if random.random() < 0.05 else 0  # 5% false positive rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic helpers\n",
    "def generate_action(role):\n",
    "    if random.random() < 0.1:  # 10% anomaly role violation\n",
    "        all_actions = set(sum(actions_allowed.values(), []))\n",
    "        disallowed = list(all_actions - set(actions_allowed[role]))\n",
    "        return random.choice(disallowed) if disallowed else random.choice(actions_allowed[role])\n",
    "    else:\n",
    "        return random.choice(actions_allowed[role])\n",
    "\n",
    "def generate_time_taken(role):\n",
    "    base = 5000\n",
    "    if role == 'Pathologist Assistant': base = 4000\n",
    "    elif role == 'Admin': base = 6000\n",
    "    if random.random() < 0.05:  # 5% too-fast anomaly\n",
    "        return random.randint(1, base // 10)\n",
    "    return random.randint(base - 1000, base + 2000)\n",
    "\n",
    "def synthetic_record(user_id, start_time):\n",
    "    role = random.choice(roles)\n",
    "    city = random.choice(cities)\n",
    "    action = generate_action(role)\n",
    "    time_taken = generate_time_taken(role)\n",
    "    user_region = random.choice(user_regions)\n",
    "    organization = random.choice(organizations)\n",
    "    audit_date = start_time + timedelta(minutes=random.randint(0, 43200))  # last 30 days\n",
    "    ip = Faker().ipv4()\n",
    "    \n",
    "    # Determine fraud status based on patterns\n",
    "    is_fraud = determine_fraud_status(role, action, time_taken, audit_date, user_region)\n",
    "    \n",
    "    return {\n",
    "        'userid': user_id,\n",
    "        'formatteddisplayname': Faker().name(),\n",
    "        'email': Faker().email(),\n",
    "        'npi': 'UNKNOWN',\n",
    "        'user region': user_region,\n",
    "        'associationtype': 'Ordering Facility',\n",
    "        'rolename': role,\n",
    "        'organizationid': random.randint(15000, 16000),\n",
    "        'caseid': random.randint(8200000, 8300000),\n",
    "        'auditid': random.randint(180000000, 190000000),\n",
    "        'actionperformed': action,\n",
    "        'actiondetails': f\"LoginId={Faker().user_name()};URL=https://lis.vitalaxis.com;IPAddress={ip};OS Version=Windows 10;Browser Version=Chrome 139;\",\n",
    "        'timetaken': time_taken,\n",
    "        'auditdate': audit_date,\n",
    "        'casestatus': 'Finalized' if random.random() > 0.4 else 'Addendum - Pending Sign-Out',\n",
    "        'audittype': 'Cases',\n",
    "        'accountid': random.randint(50000, 50050),\n",
    "        'displayname': organization,\n",
    "        'facility state': 'MA',\n",
    "        'status': 'active',\n",
    "        'orgid': random.randint(15000, 16000),\n",
    "        'organizationname': organization,\n",
    "        'city': city,\n",
    "        'is_fraud': is_fraud  # Add fraud label\n",
    "    }\n",
    "\n",
    "def generate_synthetic_dataset(record_count=2000):\n",
    "    start_time = pd.Timestamp.now() - timedelta(days=30)\n",
    "    data = []\n",
    "    user_count = record_count // 20\n",
    "    for user_id in range(1000, 1000 + user_count):\n",
    "        for _ in range(20):\n",
    "            data.append(synthetic_record(user_id, start_time))\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real or synthetic data\n",
    "try:\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "except Exception as e:\n",
    "    print(\"Using synthetic dataset (\", e, \")\")\n",
    "    df = generate_synthetic_dataset(2000)\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fraud-labeled dataset generation\n",
    "print(\"Generating synthetic dataset with fraud labels...\")\n",
    "test_df = generate_synthetic_dataset(1000)  # Generate 1000 records for testing\n",
    "\n",
    "print(f\"\\nDataset shape: {test_df.shape}\")\n",
    "print(f\"\\nColumns: {list(test_df.columns)}\")\n",
    "\n",
    "# Check fraud distribution\n",
    "fraud_counts = test_df['is_fraud'].value_counts()\n",
    "fraud_percentage = (fraud_counts[1] / len(test_df)) * 100 if 1 in fraud_counts else 0\n",
    "\n",
    "print(f\"\\nFraud Distribution:\")\n",
    "print(f\"Not Fraud (0): {fraud_counts.get(0, 0)} records\")\n",
    "print(f\"Fraud (1): {fraud_counts.get(1, 0)} records\")\n",
    "print(f\"Fraud Rate: {fraud_percentage:.2f}%\")\n",
    "\n",
    "# Show sample records\n",
    "print(f\"\\nSample records:\")\n",
    "print(test_df[['userid', 'rolename', 'actionperformed', 'timetaken', 'auditdate', 'user region', 'is_fraud']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your DataFrame is named df:\n",
    "df = df.drop(columns=['#', 'timetaken', 'displayname', 'orgid'], errors='ignore')\n",
    "# or, in-place:\n",
    "# df.drop(columns=['#', 'timetaken', 'displayname', 'orgid'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize fields\n",
    "df['auditdate'] = pd.to_datetime(df['auditdate'])\n",
    "# df['timeTakenMs'] = df['timetaken'].fillna(0).astype(int)\n",
    "df['actionperformed'] = df['actionperformed'].fillna('Unknown')\n",
    "df['rolename'] = df['rolename'].fillna('Unknown')\n",
    "df['userId'] = df['userid'].astype(int)\n",
    "df['ip'] = df['actiondetails'].str.extract(r'IPAddress=([^;]+)')[0].fillna('Unknown')\n",
    "df['city'] = df['city'].fillna('Unknown')\n",
    "\n",
    "# City lat/lng\n",
    "df['lat'] = df['city'].map(lambda c: cities_coords.get(c, (np.nan, np.nan))[0])\n",
    "df['lng'] = df['city'].map(lambda c: cities_coords.get(c, (np.nan, np.nan))[1])\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_unknown = (df['ip'].isna() | (df['ip'] == 'Unknown')).mean() * 100\n",
    "print(f\"{pct_unknown:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'ip' column\n",
    "df.drop(columns=['ip'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where caseid is 'Unknown' or NaN\n",
    "import numpy as np\n",
    "df = df.replace({'caseid': {'Unknown': np.nan}}).dropna(subset=['caseid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['caseid'] = df['caseid'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling features and flags prep\n",
    "# def is_after_hours(ts):\n",
    "#     return ts.hour < 9 or ts.hour >= 17\n",
    "\n",
    "# # Sort for rolling stats\n",
    "# df = df.sort_values(['userId', 'auditdate'])\n",
    "# df['auditdate'] = pd.to_datetime(df['auditdate'], errors='coerce')\n",
    "\n",
    "\n",
    "# R3 - actions in last 15 mins\n",
    "# df['actions_15m'] = df.groupby('userId')['auditdate'].transform(lambda x: x.rolling('15min').count())\n",
    "\n",
    "# 30-day rolling 95th percentile for R3\n",
    "# df['p95_actions_30d'] = df.groupby('userId')['actions_15m'].transform(lambda x: x.rolling('30d').quantile(0.95)).fillna(0)\n",
    "\n",
    "# # 30-day rolling minimum time for R4\n",
    "# df['min_time_ms_30d'] = df.groupby('userId')['timeTakenMs'].transform(lambda x: x.rolling('30d').min()).fillna(0)\n",
    "\n",
    "# df['after_hours'] = df['auditdate'].apply(is_after_hours)\n",
    "\n",
    "# # Location key from lat/lng (rounded to reduce jitter)\n",
    "# df['loc_key'] = (df['lat'].round(3).astype(str) + ',' + df['lng'].round(3).astype(str))\n",
    "\n",
    "# # First time this location for the user?\n",
    "# df['is_new_loc'] = ~df.groupby('userId')['loc_key'].apply(lambda s: s.duplicated(keep='first')).fillna(False)\n",
    "\n",
    "# df[['is_new_loc','after_hours']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rules R1R6\n",
    "# # Define the required variables first\n",
    "# actions_allowed = {\n",
    "#     'Pathologist Assistant': ['Logged In', 'Viewed Final Report', 'Viewed Case Information', 'Revised as Addendum',\n",
    "#                               'Added Supplemental', 'Modified Diagnosis', 'Previewed Report'],\n",
    "#     'Admin': ['Logged In', 'Viewed Final Report', 'Modified Diagnosis', 'Added Supplemental', 'Create User', 'Delete User'],\n",
    "#     'Lab Technician': ['Logged In', 'Viewed Case Information', 'Collected Sample']\n",
    "# }\n",
    "\n",
    "# sensitive_actions = {'Revised as Addendum', 'Modified Diagnosis', 'Create User', 'Delete User'}\n",
    "\n",
    "# allowed_actions = {k:set(v) for k, v in actions_allowed.items()}\n",
    "\n",
    "# # R1: Role violation\n",
    "# df['R1_flag'] = ~df.apply(lambda r: r['actionperformed'] in allowed_actions.get(r['rolename'], set()), axis=1)\n",
    "\n",
    "# # R2: New country + sensitive action - simplified as just sensitive action here\n",
    "# def r2_flag(group):\n",
    "#     seen_countries = set()\n",
    "#     flags = []\n",
    "#     for _, row in group.iterrows():\n",
    "#         new_country = row['user region'] not in seen_countries\n",
    "#         flag = new_country and (row['actionperformed'] in sensitive_actions)\n",
    "#         flags.append(flag)\n",
    "#         seen_countries.add(row['user region'])\n",
    "#     return pd.Series(flags, index=group.index)\n",
    "\n",
    "# # Fix the groupby issue by ensuring we get a Series\n",
    "# df['R2_flag'] = df.groupby('userId', group_keys=False).apply(r2_flag)\n",
    "\n",
    "# # # R3: Volume spike\n",
    "# # df['R3_flag'] = df['actions_15m'] > (df['p95_actions_30d'] + 3)\n",
    "\n",
    "# # # R4: Too fast execution (margin = 100ms)\n",
    "# # margin_ms = 100\n",
    "# # df['R4_flag'] = df['timeTakenMs'] < (df['min_time_ms_30d'] - margin_ms)\n",
    "\n",
    "# # # R5: After hours + new IP\n",
    "# # df['R5_flag'] = df['after_hours'] & df['is_new_ip']\n",
    "\n",
    "# # R6: Geo-velocity > 1500km in 30 mins\n",
    "# def geo_velocity_flag(user_df):\n",
    "#     flags = []\n",
    "#     for idx, row in user_df.iterrows():\n",
    "#         flagged = False\n",
    "#         lat1, lng1 = row['lat'], row['lng']\n",
    "#         if np.isnan(lat1) or np.isnan(lng1):\n",
    "#             flags.append(False)\n",
    "#             continue\n",
    "#         recent = user_df[(user_df['auditdate'] < row['auditdate']) &\n",
    "#                          (user_df['auditdate'] >= row['auditdate'] - timedelta(minutes=30))]\n",
    "#         for _, prev_row in recent.iterrows():\n",
    "#             lat2, lng2 = prev_row['lat'], prev_row['lng']\n",
    "#             if np.isnan(lat2) or np.isnan(lng2):\n",
    "#                 continue\n",
    "#             dist = geodesic((lat1, lng1), (lat2, lng2)).km\n",
    "#             if dist > 1500:\n",
    "#                 flagged = True\n",
    "#                 break\n",
    "#         flags.append(flagged)\n",
    "#     return pd.Series(flags, index=user_df.index)\n",
    "\n",
    "# df['R6_flag'] = df.groupby('userId', group_keys=False).apply(geo_velocity_flag)\n",
    "\n",
    "# df[[c for c in df.columns if c.endswith('_flag')]].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest - Feature Analysis for Anomaly Detection\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"\\nAvailable columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Analyze potential features for Isolation Forest\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANOMALY DETECTION FEATURES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. TEMPORAL FEATURES (Time-based anomalies)\n",
    "print(\"\\n1. TEMPORAL FEATURES:\")\n",
    "print(\"- Hour of day (df['auditdate'].dt.hour)\")\n",
    "print(\"- Day of week (df['auditdate'].dt.dayofweek)\")\n",
    "print(\"- Time between actions (if multiple actions per user)\")\n",
    "\n",
    "# 2. BEHAVIORAL FEATURES (User behavior patterns)\n",
    "print(\"\\n2. BEHAVIORAL FEATURES:\")\n",
    "print(\"- Action frequency per user\")\n",
    "print(\"- Number of different actions performed\")\n",
    "print(\"- Session duration patterns\")\n",
    "print(\"- Geographic patterns (city changes)\")\n",
    "\n",
    "# 3. ROLE-BASED FEATURES (Role-specific anomalies)\n",
    "print(\"\\n3. ROLE-BASED FEATURES:\")\n",
    "print(\"- Action count per role\")\n",
    "print(\"- Unusual action sequences for role\")\n",
    "print(\"- Time spent on actions (if available)\")\n",
    "\n",
    "# 4. GEOGRAPHIC FEATURES (Location anomalies)\n",
    "print(\"\\n4. GEOGRAPHIC FEATURES:\")\n",
    "print(\"- Latitude/Longitude coordinates\")\n",
    "print(\"- Distance between consecutive actions\")\n",
    "print(\"- New location detection\")\n",
    "\n",
    "# 5. ORGANIZATIONAL FEATURES (Organization patterns)\n",
    "print(\"\\n5. ORGANIZATIONAL FEATURES:\")\n",
    "print(\"- Organization size (number of users)\")\n",
    "print(\"- Action distribution across organizations\")\n",
    "print(\"- User region patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RECOMMENDED FEATURES FOR ISOLATION FOREST:\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Isolation Forest\n",
    "\n",
    "# 1. TEMPORAL FEATURES\n",
    "df['hour_of_day'] = df['auditdate'].dt.hour\n",
    "df['day_of_week'] = df['auditdate'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['is_after_hours'] = ((df['hour_of_day'] < 9) | (df['hour_of_day'] >= 17)).astype(int)\n",
    "\n",
    "# 2. USER BEHAVIOR FEATURES\n",
    "# Count actions per user\n",
    "user_action_counts = df.groupby('userId').size().to_dict()\n",
    "df['user_action_frequency'] = df['userId'].map(user_action_counts)\n",
    "\n",
    "# Count unique actions per user\n",
    "user_unique_actions = df.groupby('userId')['actionperformed'].nunique().to_dict()\n",
    "df['user_unique_actions'] = df['userId'].map(user_unique_actions)\n",
    "\n",
    "# Count unique cities per user (geographic diversity)\n",
    "user_cities = df.groupby('userId')['city'].nunique().to_dict()\n",
    "df['user_geographic_diversity'] = df['userId'].map(user_cities)\n",
    "\n",
    "# 3. ROLE ENCODING\n",
    "le_role = LabelEncoder()\n",
    "df['role_encoded'] = le_role.fit_transform(df['rolename'])\n",
    "\n",
    "# 4. ACTION ENCODING\n",
    "le_action = LabelEncoder()\n",
    "df['action_encoded'] = le_action.fit_transform(df['actionperformed'])\n",
    "\n",
    "# 5. ORGANIZATION FEATURES\n",
    "org_user_counts = df.groupby('organizationid').size().to_dict()\n",
    "df['org_size'] = df['organizationid'].map(org_user_counts)\n",
    "\n",
    "# 6. GEOGRAPHIC FEATURES (using lat/lng if available)\n",
    "if 'lat' in df.columns and 'lng' in df.columns:\n",
    "    df['has_coordinates'] = (~df['lat'].isna() & ~df['lng'].isna()).astype(int)\n",
    "else:\n",
    "    df['has_coordinates'] = 0\n",
    "\n",
    "print(\"Features created:\")\n",
    "print(\"- hour_of_day, day_of_week, is_weekend, is_after_hours\")\n",
    "print(\"- user_action_frequency, user_unique_actions, user_geographic_diversity\")\n",
    "print(\"- role_encoded, action_encoded\")\n",
    "print(\"- org_size, has_coordinates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest Implementation\n",
    "\n",
    "# Select features for anomaly detection\n",
    "isolation_features = [\n",
    "    'hour_of_day', 'day_of_week', 'is_weekend', 'is_after_hours',\n",
    "    'user_action_frequency', 'user_unique_actions', 'user_geographic_diversity',\n",
    "    'role_encoded', 'action_encoded', 'org_size', 'has_coordinates'\n",
    "]\n",
    "\n",
    "# Check which features are available\n",
    "available_features = [f for f in isolation_features if f in df.columns]\n",
    "print(f\"Using {len(available_features)} features for Isolation Forest:\")\n",
    "print(available_features)\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = df[available_features].copy()\n",
    "\n",
    "# Handle any missing values\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Initialize and fit Isolation Forest\n",
    "# contamination: proportion of anomalies expected (5% = 0.05)\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.1,  # Expect 10% of data to be anomalies\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "iso_forest.fit(X)\n",
    "\n",
    "# Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "anomaly_predictions = iso_forest.predict(X)\n",
    "anomaly_scores = iso_forest.decision_function(X)\n",
    "\n",
    "# Add results to dataframe\n",
    "df['isolation_forest_anomaly'] = (anomaly_predictions == -1).astype(int)\n",
    "df['isolation_forest_score'] = anomaly_scores\n",
    "\n",
    "# Results summary\n",
    "n_anomalies = df['isolation_forest_anomaly'].sum()\n",
    "n_total = len(df)\n",
    "anomaly_percentage = (n_anomalies / n_total) * 100\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"ISOLATION FOREST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total records: {n_total}\")\n",
    "print(f\"Anomalies detected: {n_anomalies}\")\n",
    "print(f\"Anomaly percentage: {anomaly_percentage:.2f}%\")\n",
    "print(f\"Normal records: {n_total - n_anomalies}\")\n",
    "\n",
    "# Show feature importance (based on average depth)\n",
    "feature_importances = np.abs(iso_forest.estimators_[0].feature_importances_)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importances:\")\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and Visualize Detected Anomalies\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ANOMALY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Anomalies by Role\n",
    "print(\"\\n1. ANOMALIES BY ROLE:\")\n",
    "role_anomalies = df.groupby('rolename')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "role_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "print(role_anomalies)\n",
    "\n",
    "# 2. Anomalies by Action Type\n",
    "print(\"\\n2. ANOMALIES BY ACTION TYPE:\")\n",
    "action_anomalies = df.groupby('actionperformed')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "action_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "action_anomalies = action_anomalies[action_anomalies['Total_Actions'] >= 2].sort_values('Anomaly_Rate', ascending=False)\n",
    "print(action_anomalies.head(10))\n",
    "\n",
    "# 3. Anomalies by Time of Day\n",
    "print(\"\\n3. ANOMALIES BY HOUR OF DAY:\")\n",
    "hour_anomalies = df.groupby('hour_of_day')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "hour_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "print(hour_anomalies)\n",
    "\n",
    "# 4. Top Anomalous Records\n",
    "print(\"\\n4. TOP 10 MOST ANOMALOUS RECORDS:\")\n",
    "anomalous_records = df[df['isolation_forest_anomaly'] == 1].nlargest(10, 'isolation_forest_score')\n",
    "display_cols = ['userId', 'rolename', 'actionperformed', 'city', 'hour_of_day', 'isolation_forest_score']\n",
    "print(anomalous_records[display_cols].to_string())\n",
    "\n",
    "# 5. Anomaly Score Distribution\n",
    "print(\"\\n5. ANOMALY SCORE DISTRIBUTION:\")\n",
    "print(f\"Minimum score: {df['isolation_forest_score'].min():.3f}\")\n",
    "print(f\"Maximum score: {df['isolation_forest_score'].max():.3f}\")\n",
    "print(f\"Mean score: {df['isolation_forest_score'].mean():.3f}\")\n",
    "print(f\"Standard deviation: {df['isolation_forest_score'].std():.3f}\")\n",
    "\n",
    "# 6. Anomalies by Organization\n",
    "print(\"\\n6. ANOMALIES BY ORGANIZATION:\")\n",
    "if 'organizationname' in df.columns:\n",
    "    org_anomalies = df.groupby('organizationname')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "    org_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "    org_anomalies = org_anomalies.sort_values('Anomaly_Rate', ascending=False)\n",
    "    print(org_anomalies.head(5))\n",
    "\n",
    "# 7. Geographic Anomalies\n",
    "print(\"\\n7. ANOMALIES BY CITY:\")\n",
    "city_anomalies = df.groupby('city')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "city_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "city_anomalies = city_anomalies[city_anomalies['Total_Actions'] >= 2].sort_values('Anomaly_Rate', ascending=False)\n",
    "print(city_anomalies.head(5))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PURE ML ANOMALY DETECTION COMPLETE!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Anomalies\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Isolation Forest Anomaly Detection Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Anomaly Score Distribution\n",
    "axes[0,0].hist(df['isolation_forest_score'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].axvline(df[df['isolation_forest_anomaly']==1]['isolation_forest_score'].min(), \n",
    "                  color='red', linestyle='--', linewidth=2, label='Anomaly Threshold')\n",
    "axes[0,0].set_title('Distribution of Anomaly Scores')\n",
    "axes[0,0].set_xlabel('Anomaly Score')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Anomalies by Hour of Day\n",
    "hour_anomaly_data = df.groupby('hour_of_day')['isolation_forest_anomaly'].mean().reset_index()\n",
    "axes[0,1].bar(hour_anomaly_data['hour_of_day'], hour_anomaly_data['isolation_forest_anomaly'], \n",
    "              color='lightcoral', alpha=0.7)\n",
    "axes[0,1].set_title('Anomaly Rate by Hour of Day')\n",
    "axes[0,1].set_xlabel('Hour of Day')\n",
    "axes[0,1].set_ylabel('Anomaly Rate')\n",
    "axes[0,1].set_xticks(range(0, 24, 2))\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Anomalies by Role\n",
    "role_anomaly_data = df.groupby('rolename')['isolation_forest_anomaly'].mean().reset_index()\n",
    "bars = axes[1,0].bar(role_anomaly_data['rolename'], role_anomaly_data['isolation_forest_anomaly'], \n",
    "                     color='lightgreen', alpha=0.7)\n",
    "axes[1,0].set_title('Anomaly Rate by Role')\n",
    "axes[1,0].set_xlabel('Role')\n",
    "axes[1,0].set_ylabel('Anomaly Rate')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scatter plot: User Action Frequency vs Anomaly Score\n",
    "scatter = axes[1,1].scatter(df['user_action_frequency'], df['isolation_forest_score'], \n",
    "                           c=df['isolation_forest_anomaly'], cmap='RdYlBu_r', alpha=0.6)\n",
    "axes[1,1].set_title('User Action Frequency vs Anomaly Score')\n",
    "axes[1,1].set_xlabel('User Action Frequency')\n",
    "axes[1,1].set_ylabel('Anomaly Score')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1,1], label='Anomaly (1=Yes, 0=No)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANOMALY DETECTION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Records Analyzed: {len(df)}\")\n",
    "print(f\"Anomalies Detected: {df['isolation_forest_anomaly'].sum()}\")\n",
    "print(f\"Anomaly Rate: {(df['isolation_forest_anomaly'].sum() / len(df) * 100):.2f}%\")\n",
    "print(f\"Normal Records: {len(df) - df['isolation_forest_anomaly'].sum()}\")\n",
    "\n",
    "# Most anomalous users\n",
    "print(f\"\\nTop 5 Most Anomalous Users:\")\n",
    "top_anomalous_users = df[df['isolation_forest_anomaly'] == 1].groupby('userId')['isolation_forest_score'].max().nlargest(5)\n",
    "for user_id, score in top_anomalous_users.items():\n",
    "    user_actions = df[df['userId'] == user_id]['actionperformed'].unique()\n",
    "    print(f\"User {user_id}: Score {score:.3f}, Actions: {list(user_actions)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
