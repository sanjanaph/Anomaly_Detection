{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall scikit-learn -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy==1.24.3 pandas==2.1.4 matplotlib==3.8.2 scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r\"C:\\Users\\Hp\\anomaly_detection\\Caseauditdetails_LIS 3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split implementation across modular cells below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from faker import Faker\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and configuration\n",
    "fake = Faker()\n",
    "roles = ['Pathologist Assistant', 'Admin', 'Lab Technician']\n",
    "actions_allowed = {\n",
    "    'Pathologist Assistant': ['Logged In', 'Viewed Final Report', 'Viewed Case Information', 'Revised as Addendum',\n",
    "                              'Added Supplemental', 'Modified Diagnosis', 'Previewed Report'],\n",
    "    'Admin': ['Logged In', 'Viewed Final Report', 'Modified Diagnosis', 'Added Supplemental', 'Create User', 'Delete User'],\n",
    "    'Lab Technician': ['Logged In', 'Viewed Case Information', 'Collected Sample']\n",
    "}\n",
    "sensitive_actions = {'Revised as Addendum', 'Modified Diagnosis', 'Create User', 'Delete User'}\n",
    "cities_coords = {\n",
    "    'Hudson': (42.3770, -71.5661),\n",
    "    'Boston': (42.3601, -71.0589),\n",
    "    'New York': (40.7128, -74.0060),\n",
    "    'Chicago': (41.8781, -87.6298),\n",
    "    'San Francisco': (37.7749, -122.4194)\n",
    "}\n",
    "cities = list(cities_coords.keys())\n",
    "user_regions = ['US', 'EU', 'ASIA']\n",
    "organizations = ['Leominster Dermatology, LLP', 'Quantum Pathology Inc', 'Health Lab Solutions']\n",
    "DATA_PATH = r\"C:\\Users\\Hp\\anomaly_detection\\Caseauditdetails_LIS 3.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic helpers\n",
    "def generate_action(role):\n",
    "    if random.random() < 0.1:  # 10% anomaly role violation\n",
    "        all_actions = set(sum(actions_allowed.values(), []))\n",
    "        disallowed = list(all_actions - set(actions_allowed[role]))\n",
    "        return random.choice(disallowed) if disallowed else random.choice(actions_allowed[role])\n",
    "    else:\n",
    "        return random.choice(actions_allowed[role])\n",
    "\n",
    "def generate_time_taken(role):\n",
    "    base = 5000\n",
    "    if role == 'Pathologist Assistant': base = 4000\n",
    "    elif role == 'Admin': base = 6000\n",
    "    if random.random() < 0.05:  # 5% too-fast anomaly\n",
    "        return random.randint(1, base // 10)\n",
    "    return random.randint(base - 1000, base + 2000)\n",
    "\n",
    "def synthetic_record(user_id, start_time):\n",
    "    role = random.choice(roles)\n",
    "    city = random.choice(cities)\n",
    "    action = generate_action(role)\n",
    "    time_taken = generate_time_taken(role)\n",
    "    user_region = random.choice(user_regions)\n",
    "    organization = random.choice(organizations)\n",
    "    audit_date = start_time + timedelta(minutes=random.randint(0, 43200))  # last 30 days\n",
    "    ip = Faker().ipv4()\n",
    "    return {\n",
    "        'userid': user_id,\n",
    "        'formatteddisplayname': Faker().name(),\n",
    "        'email': Faker().email(),\n",
    "        'npi': 'UNKNOWN',\n",
    "        'user region': user_region,\n",
    "        'associationtype': 'Ordering Facility',\n",
    "        'rolename': role,\n",
    "        'organizationid': random.randint(15000, 16000),\n",
    "        'caseid': random.randint(8200000, 8300000),\n",
    "        'auditid': random.randint(180000000, 190000000),\n",
    "        'actionperformed': action,\n",
    "        'actiondetails': f\"LoginId={Faker().user_name()};URL=https://lis.vitalaxis.com;IPAddress={ip};OS Version=Windows 10;Browser Version=Chrome 139;\",\n",
    "        'timetaken': time_taken,\n",
    "        'auditdate': audit_date,\n",
    "        'casestatus': 'Finalized' if random.random() > 0.4 else 'Addendum - Pending Sign-Out',\n",
    "        'audittype': 'Cases',\n",
    "        'accountid': random.randint(50000, 50050),\n",
    "        'displayname': organization,\n",
    "        'facility state': 'MA',\n",
    "        'status': 'active',\n",
    "        'orgid': random.randint(15000, 16000),\n",
    "        'organizationname': organization,\n",
    "        'city': city\n",
    "    }\n",
    "\n",
    "def generate_synthetic_dataset(record_count=2000):\n",
    "    start_time = pd.Timestamp.now() - timedelta(days=30)\n",
    "    data = []\n",
    "    user_count = record_count // 20\n",
    "    for user_id in range(1000, 1000 + user_count):\n",
    "        for _ in range(20):\n",
    "            data.append(synthetic_record(user_id, start_time))\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real or synthetic data\n",
    "try:\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "except Exception as e:\n",
    "    print(\"Using synthetic dataset (\", e, \")\")\n",
    "    df = generate_synthetic_dataset(2000)\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your DataFrame is named df:\n",
    "df = df.drop(columns=['#', 'timetaken', 'displayname', 'orgid'], errors='ignore')\n",
    "# or, in-place:\n",
    "# df.drop(columns=['#', 'timetaken', 'displayname', 'orgid'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize fields\n",
    "df['auditdate'] = pd.to_datetime(df['auditdate'])\n",
    "# df['timeTakenMs'] = df['timetaken'].fillna(0).astype(int)\n",
    "df['actionperformed'] = df['actionperformed'].fillna('Unknown')\n",
    "df['rolename'] = df['rolename'].fillna('Unknown')\n",
    "df['userId'] = df['userid'].astype(int)\n",
    "df['ip'] = df['actiondetails'].str.extract(r'IPAddress=([^;]+)')[0].fillna('Unknown')\n",
    "df['city'] = df['city'].fillna('Unknown')\n",
    "\n",
    "# City lat/lng\n",
    "df['lat'] = df['city'].map(lambda c: cities_coords.get(c, (np.nan, np.nan))[0])\n",
    "df['lng'] = df['city'].map(lambda c: cities_coords.get(c, (np.nan, np.nan))[1])\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_unknown = (df['ip'].isna() | (df['ip'] == 'Unknown')).mean() * 100\n",
    "print(f\"{pct_unknown:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'ip' column\n",
    "df.drop(columns=['ip'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where caseid is 'Unknown' or NaN\n",
    "import numpy as np\n",
    "df = df.replace({'caseid': {'Unknown': np.nan}}).dropna(subset=['caseid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['caseid'] = df['caseid'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling features and flags prep\n",
    "# def is_after_hours(ts):\n",
    "#     return ts.hour < 9 or ts.hour >= 17\n",
    "\n",
    "# # Sort for rolling stats\n",
    "# df = df.sort_values(['userId', 'auditdate'])\n",
    "# df['auditdate'] = pd.to_datetime(df['auditdate'], errors='coerce')\n",
    "\n",
    "\n",
    "# R3 - actions in last 15 mins\n",
    "# df['actions_15m'] = df.groupby('userId')['auditdate'].transform(lambda x: x.rolling('15min').count())\n",
    "\n",
    "# 30-day rolling 95th percentile for R3\n",
    "# df['p95_actions_30d'] = df.groupby('userId')['actions_15m'].transform(lambda x: x.rolling('30d').quantile(0.95)).fillna(0)\n",
    "\n",
    "# # 30-day rolling minimum time for R4\n",
    "# df['min_time_ms_30d'] = df.groupby('userId')['timeTakenMs'].transform(lambda x: x.rolling('30d').min()).fillna(0)\n",
    "\n",
    "# df['after_hours'] = df['auditdate'].apply(is_after_hours)\n",
    "\n",
    "# # Location key from lat/lng (rounded to reduce jitter)\n",
    "# df['loc_key'] = (df['lat'].round(3).astype(str) + ',' + df['lng'].round(3).astype(str))\n",
    "\n",
    "# # First time this location for the user?\n",
    "# df['is_new_loc'] = ~df.groupby('userId')['loc_key'].apply(lambda s: s.duplicated(keep='first')).fillna(False)\n",
    "\n",
    "# df[['is_new_loc','after_hours']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rules R1â€“R6\n",
    "# # Define the required variables first\n",
    "# actions_allowed = {\n",
    "#     'Pathologist Assistant': ['Logged In', 'Viewed Final Report', 'Viewed Case Information', 'Revised as Addendum',\n",
    "#                               'Added Supplemental', 'Modified Diagnosis', 'Previewed Report'],\n",
    "#     'Admin': ['Logged In', 'Viewed Final Report', 'Modified Diagnosis', 'Added Supplemental', 'Create User', 'Delete User'],\n",
    "#     'Lab Technician': ['Logged In', 'Viewed Case Information', 'Collected Sample']\n",
    "# }\n",
    "\n",
    "# sensitive_actions = {'Revised as Addendum', 'Modified Diagnosis', 'Create User', 'Delete User'}\n",
    "\n",
    "# allowed_actions = {k:set(v) for k, v in actions_allowed.items()}\n",
    "\n",
    "# # R1: Role violation\n",
    "# df['R1_flag'] = ~df.apply(lambda r: r['actionperformed'] in allowed_actions.get(r['rolename'], set()), axis=1)\n",
    "\n",
    "# # R2: New country + sensitive action - simplified as just sensitive action here\n",
    "# def r2_flag(group):\n",
    "#     seen_countries = set()\n",
    "#     flags = []\n",
    "#     for _, row in group.iterrows():\n",
    "#         new_country = row['user region'] not in seen_countries\n",
    "#         flag = new_country and (row['actionperformed'] in sensitive_actions)\n",
    "#         flags.append(flag)\n",
    "#         seen_countries.add(row['user region'])\n",
    "#     return pd.Series(flags, index=group.index)\n",
    "\n",
    "# # Fix the groupby issue by ensuring we get a Series\n",
    "# df['R2_flag'] = df.groupby('userId', group_keys=False).apply(r2_flag)\n",
    "\n",
    "# # # R3: Volume spike\n",
    "# # df['R3_flag'] = df['actions_15m'] > (df['p95_actions_30d'] + 3)\n",
    "\n",
    "# # # R4: Too fast execution (margin = 100ms)\n",
    "# # margin_ms = 100\n",
    "# # df['R4_flag'] = df['timeTakenMs'] < (df['min_time_ms_30d'] - margin_ms)\n",
    "\n",
    "# # # R5: After hours + new IP\n",
    "# # df['R5_flag'] = df['after_hours'] & df['is_new_ip']\n",
    "\n",
    "# # R6: Geo-velocity > 1500km in 30 mins\n",
    "# def geo_velocity_flag(user_df):\n",
    "#     flags = []\n",
    "#     for idx, row in user_df.iterrows():\n",
    "#         flagged = False\n",
    "#         lat1, lng1 = row['lat'], row['lng']\n",
    "#         if np.isnan(lat1) or np.isnan(lng1):\n",
    "#             flags.append(False)\n",
    "#             continue\n",
    "#         recent = user_df[(user_df['auditdate'] < row['auditdate']) &\n",
    "#                          (user_df['auditdate'] >= row['auditdate'] - timedelta(minutes=30))]\n",
    "#         for _, prev_row in recent.iterrows():\n",
    "#             lat2, lng2 = prev_row['lat'], prev_row['lng']\n",
    "#             if np.isnan(lat2) or np.isnan(lng2):\n",
    "#                 continue\n",
    "#             dist = geodesic((lat1, lng1), (lat2, lng2)).km\n",
    "#             if dist > 1500:\n",
    "#                 flagged = True\n",
    "#                 break\n",
    "#         flags.append(flagged)\n",
    "#     return pd.Series(flags, index=user_df.index)\n",
    "\n",
    "# df['R6_flag'] = df.groupby('userId', group_keys=False).apply(geo_velocity_flag)\n",
    "\n",
    "# df[[c for c in df.columns if c.endswith('_flag')]].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest - Feature Analysis for Anomaly Detection\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"\\nAvailable columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Analyze potential features for Isolation Forest\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANOMALY DETECTION FEATURES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. TEMPORAL FEATURES (Time-based anomalies)\n",
    "print(\"\\n1. TEMPORAL FEATURES:\")\n",
    "print(\"- Hour of day (df['auditdate'].dt.hour)\")\n",
    "print(\"- Day of week (df['auditdate'].dt.dayofweek)\")\n",
    "print(\"- Time between actions (if multiple actions per user)\")\n",
    "\n",
    "# 2. BEHAVIORAL FEATURES (User behavior patterns)\n",
    "print(\"\\n2. BEHAVIORAL FEATURES:\")\n",
    "print(\"- Action frequency per user\")\n",
    "print(\"- Number of different actions performed\")\n",
    "print(\"- Session duration patterns\")\n",
    "print(\"- Geographic patterns (city changes)\")\n",
    "\n",
    "# 3. ROLE-BASED FEATURES (Role-specific anomalies)\n",
    "print(\"\\n3. ROLE-BASED FEATURES:\")\n",
    "print(\"- Action count per role\")\n",
    "print(\"- Unusual action sequences for role\")\n",
    "print(\"- Time spent on actions (if available)\")\n",
    "\n",
    "# 4. GEOGRAPHIC FEATURES (Location anomalies)\n",
    "print(\"\\n4. GEOGRAPHIC FEATURES:\")\n",
    "print(\"- Latitude/Longitude coordinates\")\n",
    "print(\"- Distance between consecutive actions\")\n",
    "print(\"- New location detection\")\n",
    "\n",
    "# 5. ORGANIZATIONAL FEATURES (Organization patterns)\n",
    "print(\"\\n5. ORGANIZATIONAL FEATURES:\")\n",
    "print(\"- Organization size (number of users)\")\n",
    "print(\"- Action distribution across organizations\")\n",
    "print(\"- User region patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RECOMMENDED FEATURES FOR ISOLATION FOREST:\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Isolation Forest\n",
    "\n",
    "# 1. TEMPORAL FEATURES\n",
    "df['hour_of_day'] = df['auditdate'].dt.hour\n",
    "df['day_of_week'] = df['auditdate'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['is_after_hours'] = ((df['hour_of_day'] < 9) | (df['hour_of_day'] >= 17)).astype(int)\n",
    "\n",
    "# 2. USER BEHAVIOR FEATURES\n",
    "# Count actions per user\n",
    "user_action_counts = df.groupby('userId').size().to_dict()\n",
    "df['user_action_frequency'] = df['userId'].map(user_action_counts)\n",
    "\n",
    "# Count unique actions per user\n",
    "user_unique_actions = df.groupby('userId')['actionperformed'].nunique().to_dict()\n",
    "df['user_unique_actions'] = df['userId'].map(user_unique_actions)\n",
    "\n",
    "# Count unique cities per user (geographic diversity)\n",
    "user_cities = df.groupby('userId')['city'].nunique().to_dict()\n",
    "df['user_geographic_diversity'] = df['userId'].map(user_cities)\n",
    "\n",
    "# 3. ROLE ENCODING\n",
    "le_role = LabelEncoder()\n",
    "df['role_encoded'] = le_role.fit_transform(df['rolename'])\n",
    "\n",
    "# 4. ACTION ENCODING\n",
    "le_action = LabelEncoder()\n",
    "df['action_encoded'] = le_action.fit_transform(df['actionperformed'])\n",
    "\n",
    "# 5. ORGANIZATION FEATURES\n",
    "org_user_counts = df.groupby('organizationid').size().to_dict()\n",
    "df['org_size'] = df['organizationid'].map(org_user_counts)\n",
    "\n",
    "# 6. GEOGRAPHIC FEATURES (using lat/lng if available)\n",
    "if 'lat' in df.columns and 'lng' in df.columns:\n",
    "    df['has_coordinates'] = (~df['lat'].isna() & ~df['lng'].isna()).astype(int)\n",
    "else:\n",
    "    df['has_coordinates'] = 0\n",
    "\n",
    "print(\"Features created:\")\n",
    "print(\"- hour_of_day, day_of_week, is_weekend, is_after_hours\")\n",
    "print(\"- user_action_frequency, user_unique_actions, user_geographic_diversity\")\n",
    "print(\"- role_encoded, action_encoded\")\n",
    "print(\"- org_size, has_coordinates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest Implementation\n",
    "\n",
    "# Select features for anomaly detection\n",
    "isolation_features = [\n",
    "    'hour_of_day', 'day_of_week', 'is_weekend', 'is_after_hours',\n",
    "    'user_action_frequency', 'user_unique_actions', 'user_geographic_diversity',\n",
    "    'role_encoded', 'action_encoded', 'org_size', 'has_coordinates'\n",
    "]\n",
    "\n",
    "# Check which features are available\n",
    "available_features = [f for f in isolation_features if f in df.columns]\n",
    "print(f\"Using {len(available_features)} features for Isolation Forest:\")\n",
    "print(available_features)\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = df[available_features].copy()\n",
    "\n",
    "# Handle any missing values\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Initialize and fit Isolation Forest\n",
    "# contamination: proportion of anomalies expected (5% = 0.05)\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.1,  # Expect 10% of data to be anomalies\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "iso_forest.fit(X)\n",
    "\n",
    "# Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "anomaly_predictions = iso_forest.predict(X)\n",
    "anomaly_scores = iso_forest.decision_function(X)\n",
    "\n",
    "# Add results to dataframe\n",
    "df['isolation_forest_anomaly'] = (anomaly_predictions == -1).astype(int)\n",
    "df['isolation_forest_score'] = anomaly_scores\n",
    "\n",
    "# Results summary\n",
    "n_anomalies = df['isolation_forest_anomaly'].sum()\n",
    "n_total = len(df)\n",
    "anomaly_percentage = (n_anomalies / n_total) * 100\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"ISOLATION FOREST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total records: {n_total}\")\n",
    "print(f\"Anomalies detected: {n_anomalies}\")\n",
    "print(f\"Anomaly percentage: {anomaly_percentage:.2f}%\")\n",
    "print(f\"Normal records: {n_total - n_anomalies}\")\n",
    "\n",
    "# Show feature importance (based on average depth)\n",
    "feature_importances = np.abs(iso_forest.estimators_[0].feature_importances_)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importances:\")\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and Visualize Detected Anomalies\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ANOMALY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Anomalies by Role\n",
    "print(\"\\n1. ANOMALIES BY ROLE:\")\n",
    "role_anomalies = df.groupby('rolename')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "role_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "print(role_anomalies)\n",
    "\n",
    "# 2. Anomalies by Action Type\n",
    "print(\"\\n2. ANOMALIES BY ACTION TYPE:\")\n",
    "action_anomalies = df.groupby('actionperformed')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "action_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "action_anomalies = action_anomalies[action_anomalies['Total_Actions'] >= 2].sort_values('Anomaly_Rate', ascending=False)\n",
    "print(action_anomalies.head(10))\n",
    "\n",
    "# 3. Anomalies by Time of Day\n",
    "print(\"\\n3. ANOMALIES BY HOUR OF DAY:\")\n",
    "hour_anomalies = df.groupby('hour_of_day')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "hour_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "print(hour_anomalies)\n",
    "\n",
    "# 4. Top Anomalous Records\n",
    "print(\"\\n4. TOP 10 MOST ANOMALOUS RECORDS:\")\n",
    "anomalous_records = df[df['isolation_forest_anomaly'] == 1].nlargest(10, 'isolation_forest_score')\n",
    "display_cols = ['userId', 'rolename', 'actionperformed', 'city', 'hour_of_day', 'isolation_forest_score']\n",
    "print(anomalous_records[display_cols].to_string())\n",
    "\n",
    "# 5. Anomaly Score Distribution\n",
    "print(\"\\n5. ANOMALY SCORE DISTRIBUTION:\")\n",
    "print(f\"Minimum score: {df['isolation_forest_score'].min():.3f}\")\n",
    "print(f\"Maximum score: {df['isolation_forest_score'].max():.3f}\")\n",
    "print(f\"Mean score: {df['isolation_forest_score'].mean():.3f}\")\n",
    "print(f\"Standard deviation: {df['isolation_forest_score'].std():.3f}\")\n",
    "\n",
    "# 6. Anomalies by Organization\n",
    "print(\"\\n6. ANOMALIES BY ORGANIZATION:\")\n",
    "if 'organizationname' in df.columns:\n",
    "    org_anomalies = df.groupby('organizationname')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "    org_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "    org_anomalies = org_anomalies.sort_values('Anomaly_Rate', ascending=False)\n",
    "    print(org_anomalies.head(5))\n",
    "\n",
    "# 7. Geographic Anomalies\n",
    "print(\"\\n7. ANOMALIES BY CITY:\")\n",
    "city_anomalies = df.groupby('city')['isolation_forest_anomaly'].agg(['count', 'sum', 'mean']).round(3)\n",
    "city_anomalies.columns = ['Total_Actions', 'Anomalies', 'Anomaly_Rate']\n",
    "city_anomalies = city_anomalies[city_anomalies['Total_Actions'] >= 2].sort_values('Anomaly_Rate', ascending=False)\n",
    "print(city_anomalies.head(5))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PURE ML ANOMALY DETECTION COMPLETE!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Anomalies\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Isolation Forest Anomaly Detection Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Anomaly Score Distribution\n",
    "axes[0,0].hist(df['isolation_forest_score'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].axvline(df[df['isolation_forest_anomaly']==1]['isolation_forest_score'].min(), \n",
    "                  color='red', linestyle='--', linewidth=2, label='Anomaly Threshold')\n",
    "axes[0,0].set_title('Distribution of Anomaly Scores')\n",
    "axes[0,0].set_xlabel('Anomaly Score')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Anomalies by Hour of Day\n",
    "hour_anomaly_data = df.groupby('hour_of_day')['isolation_forest_anomaly'].mean().reset_index()\n",
    "axes[0,1].bar(hour_anomaly_data['hour_of_day'], hour_anomaly_data['isolation_forest_anomaly'], \n",
    "              color='lightcoral', alpha=0.7)\n",
    "axes[0,1].set_title('Anomaly Rate by Hour of Day')\n",
    "axes[0,1].set_xlabel('Hour of Day')\n",
    "axes[0,1].set_ylabel('Anomaly Rate')\n",
    "axes[0,1].set_xticks(range(0, 24, 2))\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Anomalies by Role\n",
    "role_anomaly_data = df.groupby('rolename')['isolation_forest_anomaly'].mean().reset_index()\n",
    "bars = axes[1,0].bar(role_anomaly_data['rolename'], role_anomaly_data['isolation_forest_anomaly'], \n",
    "                     color='lightgreen', alpha=0.7)\n",
    "axes[1,0].set_title('Anomaly Rate by Role')\n",
    "axes[1,0].set_xlabel('Role')\n",
    "axes[1,0].set_ylabel('Anomaly Rate')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scatter plot: User Action Frequency vs Anomaly Score\n",
    "scatter = axes[1,1].scatter(df['user_action_frequency'], df['isolation_forest_score'], \n",
    "                           c=df['isolation_forest_anomaly'], cmap='RdYlBu_r', alpha=0.6)\n",
    "axes[1,1].set_title('User Action Frequency vs Anomaly Score')\n",
    "axes[1,1].set_xlabel('User Action Frequency')\n",
    "axes[1,1].set_ylabel('Anomaly Score')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1,1], label='Anomaly (1=Yes, 0=No)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANOMALY DETECTION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Records Analyzed: {len(df)}\")\n",
    "print(f\"Anomalies Detected: {df['isolation_forest_anomaly'].sum()}\")\n",
    "print(f\"Anomaly Rate: {(df['isolation_forest_anomaly'].sum() / len(df) * 100):.2f}%\")\n",
    "print(f\"Normal Records: {len(df) - df['isolation_forest_anomaly'].sum()}\")\n",
    "\n",
    "# Most anomalous users\n",
    "print(f\"\\nTop 5 Most Anomalous Users:\")\n",
    "top_anomalous_users = df[df['isolation_forest_anomaly'] == 1].groupby('userId')['isolation_forest_score'].max().nlargest(5)\n",
    "for user_id, score in top_anomalous_users.items():\n",
    "    user_actions = df[df['userId'] == user_id]['actionperformed'].unique()\n",
    "    print(f\"User {user_id}: Score {score:.3f}, Actions: {list(user_actions)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
